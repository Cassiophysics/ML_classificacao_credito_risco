# Machine Learning - Classifica√ß√£o do Risco de Cr√©dito

![capa_ml_rlg](https://github.com/Cassiophysics/ML_classificacao_credito_risco/assets/108491443/4bb91092-6948-48e5-ba8d-945fbffeafd3)

## Teste voc√™ mesmo o modelo: [üí≥ SISTEMA DE APROVA√á√ÉO DE EMPR√âSTIMOS](https://cassiophysics-ml-classificacao-credito-risc-streamlitapp-8jj1cb.streamlit.app/)

Este √© um projeto de Machine Learning de aprendizado supervisionado que visa a elabora√ß√£o de um modelo capaz de classificar quando um empr√©stimo foi bom e quando foi ruim.  Para tal finalidade, o conjunto de dados utilizado foi obtido a partir do site [KAGGLE](https://www.kaggle.com/datasets/uciml/german-credit), mas tamb√©m pode ser encontrado em  [UCI](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)). Este Dataset cont√©m uma gama de atributos de clientes a fim de se prever o risco do empr√©stimo.

## Motiva√ß√£o:

O setor de empr√©stimos desempenha um papel crucial na economia, fornecendo recursos financeiros para indiv√≠duos e empresas realizarem seus objetivos e projetos. No entanto, √© essencial que os empr√©stimos sejam concedidos de forma respons√°vel, considerando os riscos envolvidos e minimizando o impacto de poss√≠veis inadimpl√™ncias.

Diante desse desafio, surge a necessidade de desenvolver abordagens inovadoras para auxiliar institui√ß√µes financeiras, bancos e empresas de cr√©dito na avalia√ß√£o de solicita√ß√µes de empr√©stimos. √â nesse contexto que se insere o projeto de classifica√ß√£o de empr√©stimos que desenvolvi.

O objetivo desse projeto √© utilizar t√©cnicas de machine learning para criar um modelo capaz de classificar se um empr√©stimo √© considerado bom ou ruim com base em informa√ß√µes e dados dispon√≠veis. A classifica√ß√£o correta dos empr√©stimos permite que as institui√ß√µes financeiras tomem decis√µes mais informadas, minimizem riscos e melhorem a efici√™ncia dos processos de concess√£o de cr√©dito.

Para alcan√ßar esse objetivo, foram explorados algoritmos de aprendizado de m√°quina, t√©cnicas de pr√©-processamento de dados e valida√ß√£o de modelos. A constru√ß√£o desse modelo envolveu etapas cruciais, como a coleta e prepara√ß√£o dos dados relevantes e a escolha do algoritmo de classifica√ß√£o mais adequado.

Ao desenvolver esse projeto, busquei n√£o apenas aprimorar minhas habilidades em modelagem de machine learning, mas tamb√©m contribuir para o avan√ßo da √°rea de cr√©dito e finan√ßas. O modelo criado possui o potencial de otimizar o processo de tomada de decis√£o, identificando padr√µes e caracter√≠sticas relevantes para a classifica√ß√£o de empr√©stimos, proporcionando uma avalia√ß√£o mais precisa e confi√°vel.

Ao disponibilizar esse projeto no GitHub, espero compartilhar conhecimento e incentivar a colabora√ß√£o com outros profissionais e entusiastas da √°rea. Al√©m disso, acredito que essa iniciativa pode contribuir para a transpar√™ncia e aprimoramento dos processos de concess√£o de cr√©dito, beneficiando tanto as institui√ß√µes financeiras quanto os clientes em busca de empr√©stimos.

Estou entusiasmado com os resultados alcan√ßados neste projeto e confiante de que ele pode ter um impacto positivo no setor financeiro. Convido voc√™ a explorar o c√≥digo, os dados e os resultados apresentados neste reposit√≥rio e a se juntar a mim nessa jornada em busca de solu√ß√µes inovadoras para o mercado de empr√©stimos.

Neste projeto foi feito o uso de Pipelines para dar maior legibilidade ao c√≥digo, facilitar a leitura do c√≥digo, for√ßar a execu√ß√£o das transforma√ß√µes na ordem correta e tornar o script mais reproduz√≠vel. E as m√©tricas definidas para a escolha do modelo mais adequado foram:

- Maior valor de recall
- Maior valor de f1-score
- A diferen√ßa entre falsos positivos e falsos negativos n√£o ser acima de 50, de modo a manter um certo equil√≠brio.

Assim, o racioc√≠nio utilizado para este empreendimento foi sistematizado em:

## **1. An√°lise Explorat√≥ria**

Nesta etapa foi utilizado com conjunto de t√©cnicas como, por exemplo, gr√°ficos e tabelas com o intuito de tentar compreender os dados e os seus padr√µes de uma maneira mais profunda. Esta √© uma etapa importante no processo de an√°lise de dados, pois permite a gera√ß√£o de insights valiosos sobre os dados e tamb√©m verificar se os dados est√£o aptos para serem usados para responder o problema de neg√≥cio em quest√£o. Neste projeto, atrav√©s da an√°lise explorat√≥ria feita foi poss√≠vel constatar que o Dataset utilizado n√£o cont√©m valores duplicados, mas possui valores nulos e outliers, assim como diversas rela√ß√µes diferentes entre os atributos.

## **2. Pr√©-processamento**

Dados sujos ou imprecisos podem levar a resultados incorretos, ou enganosos, por isso o pr√©-processamento dos dados √© uma etapa importante, pois tem como intuito realizar procedimentos para garantir que os dados estejam prontos para a elabora√ß√£o dos modelos de machine learning e que os resultados sejam confi√°veis. √Ä vista disso, fizemos o tratamento de valores nulos preenchendo pelo valor mais frequente no conjunto de dados, porque as colunas que continham valores ausentes eram categ√≥ricas, tamb√©m realizamos a padroniza√ß√£o dos dados convertendo para a mesma escala e a codifica√ß√£o das colunas categ√≥ricas.

## **3. Tentativa de Tratamento do Desbalanceamento das Classes com Diferentes Pesos**

Foi testado diferentes valores de pesos para a classe de menor frequ√™ncia, que no caso √© a de empr√©stimos ruins, tendo em vista encontrar os melhores resultados conforme as m√©tricas pr√©-definidas. Primeiro verificamos com Class Weight = 'balanced' nos algoritmos que possuem este par√¢metro, em seguida foi utilizando GridSearchCV para encontrar o melhor peso, e por fim fizemos um loop testando diversos pesos diferentes.

## **4. Tentativa de Tratamento do Desbalanceamento das Classes com SMOTE**

Ao ter conjuntos de dados com classes desbalanceadas pode-se levar a modelos de machine learning com desempenho ruim para a classe minorit√°ria. Diante disso, foi utilizada a t√©cnica SMOTE que prop√µe criar exemplos sint√©ticos da classe minorit√°ria, no qual funciona selecionando exemplos da classe minorit√°ria e encontra o k-√©simo vizinho mais pr√≥ximo para cada um deles. Em seguida, ele cria um exemplo sint√©tico no espa√ßo de caracter√≠sticas, colocando-o a meio caminho entre o exemplo selecionado e seu vizinho mais pr√≥ximo. Isso √© repetido at√© que a classe minorit√°ria tenha o mesmo tamanho da classe majorit√°ria. Isto posto, √© importante ter em mente que em alguns casos o SMOTE pode levar a uma perda de precis√£o e a um overfitting do modelo. No nosso caso, a t√≠tulo de exemplo, o tratamento de desbalanceamento testando diferentes pesos disp√¥s de melhores resultados comparado com o SMOTE.

## **5. Otimiza√ß√£o de Hiperpar√¢metros**

A otimiza√ß√£o de hiperpar√¢metros √© importante porque diferentes conjuntos de hiperpar√¢metros podem resultar em modelos com desempenhos significativamente melhores. Al√©m disso, os hiperpar√¢metros podem afetar a velocidade do treinamento do modelo e a capacidade de generaliza√ß√£o do modelo para novos dados. Para tal finalidade, foi utilizado GridSearchCV e RandomizedSearchCV para obter os hiperpar√¢metros mais adequados. Entretanto, como tivemos que definir uma √∫nica m√©trica previamente, os melhores hiperpar√¢metros encontrados consideram somente essa m√©trica, o que n√£o leva aos melhores resultados segundo os crit√©rios definidos neste projeto.

## Resultado Final do Modelo:

Conforme os crit√©rios que definimos, o modelo baseline LogisticRegression com peso 3.22 para a classe 1, apresentou o melhor resultado, com recall 0.80, f1-score 0.59, diferen√ßa entre falsos positivos e falsos negativos, igual a 50 e uma acur√°cia de 0.61. Ou seja, de todos os empr√©stimos presentes na amostra que realmente foram ruins (Falsos Negativos) nosso modelo conseguiu acertar 80%, mas tamb√©m manteve um n√∫mero razo√°vel de empr√©stimos bons classificados erroneamente como ruins (Falsos Positivos). A escolha do modelo ideal deve ser feita conforme as m√©tricas de neg√≥cio estabelecidas. Se o custo de perder um empr√©stimo ruim supera em muito o custo de cancelar v√°rios empr√©stimos leg√≠timos, ou seja, falsos positivos, talvez possamos escolher um peso que nos d√™ uma taxa de recall mais alta. Isso ocorre porque aumentamos nossa pontua√ß√£o de recall de maus empr√©stimos √† custa de mais casos leg√≠timos mal classificados.

## Impacto nos neg√≥cios:

**Melhoria da tomada de decis√µes:** O modelo de classifica√ß√£o empr√©stimo bom/ruim oferece uma ferramenta confi√°vel para avaliar a viabilidade de empr√©stimos, reduzindo o risco de concess√µes a clientes com maior probabilidade de inadimpl√™ncia.

**Redu√ß√£o de riscos e perdas financeiras:** O modelo identifica empr√©stimos de alto risco antecipadamente, permitindo medidas preventivas, como limites de cr√©dito mais baixos ou recusa do empr√©stimo, reduzindo as perdas financeiras.

**Aumento da efici√™ncia operacional:** O processo automatizado acelera a avalia√ß√£o de cr√©dito, economizando tempo e recursos, direcionando a equipe para atividades estrat√©gicas.

**Melhoria da experi√™ncia do cliente:** Com uma avalia√ß√£o precisa, a empresa pode oferecer melhores condi√ß√µes de empr√©stimo a clientes confi√°veis, melhorando sua experi√™ncia e fortalecendo o relacionamento.

**Aprimoramento da gest√£o de riscos:** O modelo fornece insights sobre fatores que influenciam a qualidade do empr√©stimo, melhorando as estrat√©gias de gerenciamento de riscos e a sa√∫de financeira.

**Vantagem competitiva:** A implementa√ß√£o do modelo diferencia a empresa, atraindo mais clientes e fortalecendo sua reputa√ß√£o no mercado.

## Identifica√ß√£o de melhorias para o modelo:

**Aumentar o tamanho e a qualidade do conjunto de dados:** Coletar mais dados de alta qualidade para enriquecer a variabilidade e representatividade das amostras.

**Feature Engineering:** Analisar de forma mais detalhada as caracter√≠sticas existentes e identificar se h√° oportunidades para criar novas vari√°veis ou transformar as existentes de maneira mais informativa. Isso pode envolver a combina√ß√£o de vari√°veis, cria√ß√£o de vari√°veis ‚Äã‚Äãinterativas, extra√ß√£o de caracter√≠sticas relevantes ou at√© mesmo a utiliza√ß√£o de t√©cnicas avan√ßadas como redu√ß√£o de dimensionalidade.

**Regulariza√ß√£o:** Considerar a aplica√ß√£o de t√©cnicas de regulariza√ß√£o, como a penaliza√ß√£o L1 ou L2, para evitar o overfitting e melhorar a generaliza√ß√£o do modelo. Isso pode ajudar a controlar a complexidade do modelo e reduzir a sensibilidade a outliers ou ru√≠dos nos dados.

**An√°lise de Res√≠duos:** Realizar uma an√°lise detalhada dos res√≠duos do modelo para identificar poss√≠veis padr√µes n√£o capturados ou problemas de modelagem. A an√°lise de res√≠duos pode ajudar a identificar √°reas de melhoria, como a inclus√£o de vari√°veis ‚Äã‚Äãrelevantes ou a aplica√ß√£o de transforma√ß√µes adicionais.

**Monitoramento e Retreinamento:** Estabelecer um processo de monitoramento cont√≠nuo do desempenho do seu modelo em produ√ß√£o. Se poss√≠vel, coletar novos dados e reavaliar regularmente o desempenho do modelo. Isso permitir√° identificar mudan√ßas nos padr√µes ou no comportamento dos clientes ao longo do tempo e garantir que o modelo permane√ßa atualizado e relevante.




